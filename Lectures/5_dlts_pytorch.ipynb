{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b82071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154addb",
   "metadata": {},
   "source": [
    "# Opérations sur les tenseurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799502f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'un tenseur\n",
    "tenseur = torch.zeros((10,5,100)) # [10,5,100]\n",
    "tenseur = torch.ones((10,5,100)) # [10,5,100]\n",
    "tenseur = torch.rand((10,5,100)) # [10,5,100]\n",
    "\n",
    "tenseur_zeros = torch.zeros_like(tenseur) # [10,5,100]\n",
    "tenseur_copy = torch.clone(tenseur)\n",
    "\n",
    "print(f'Tenseur de dimensions {tenseur.shape} et de type {tenseur.dtype}')\n",
    "\n",
    "# Opération élémentaires\n",
    "tenseur2 = torch.rand((10,5,100))  # [10,5,100]\n",
    "tenseur_au_carre = tenseur**2  # [10,5,100]\n",
    "tenseur_exp = torch.exp(tenseur)  # [10,5,100]\n",
    "tenseur_somme = tenseur + tenseur2  # [10,5,100]\n",
    "tenseur_produit = tenseur * tenseur2  # [10,5,100]\n",
    "\n",
    "# Concaténation\n",
    "tenseur_concatene_axe0 = torch.cat([tenseur, tenseur2],axis=0)  # [20,5,100]\n",
    "tenseur_concatene_axe1 = torch.cat([tenseur, tenseur2],axis=1)  # [10,10,100]\n",
    "tenseur_concatene_axe2 = torch.cat([tenseur, tenseur2],axis=2)  # [10,5,200]\n",
    "\n",
    "tenseur_tile = torch.tile(tenseur,dims=(2,3,4))  # [20,15,400]\n",
    "\n",
    "# Opérations de réduction\n",
    "tenseur_reduit_axe0 = torch.mean(tenseur,axis=0)  # [5,100]\n",
    "tenseur_reduit_axe1 = torch.mean(tenseur,axis=1)  # [10,100]\n",
    "tenseur_reduit_axe2 = torch.mean(tenseur,axis=2)  # [10,5]\n",
    "\n",
    "tenseur_reduit_axe0_kd = torch.mean(tenseur,axis=0,keepdim=True)  # [1,5,100]\n",
    "\n",
    "# Manipulation sur les dimensions \n",
    "\n",
    "tenseur_21 = torch.transpose(tenseur,1,2) # [10,100,5]\n",
    "tenseur_flat = torch.flatten(tenseur) # [5000,]\n",
    "tenseur_newdim_0 = torch.unsqueeze(tenseur,dim=0) # [1,10,5,100]\n",
    "tenseur_newdim_1 = torch.unsqueeze(tenseur,dim=1) # [10,1,5,100]\n",
    "\n",
    "# Slicing \n",
    "tenseur_partie = tenseur[0,:,:] # [5,100]\n",
    "tenseur_partie = tenseur[0,...] # [5,100]\n",
    "tenseur_partie = tenseur[0:5,:,:] # [5,5,100]\n",
    "tenseur_partie = tenseur[0,:,0::2]# [10,5,50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531217b",
   "metadata": {},
   "source": [
    "# Optimisation d'un graphe de calculs\n",
    "Exemple : régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_true = 2.0\n",
    "b_true = -1.0\n",
    "\n",
    "x = torch.rand((10,)) # simulation des données\n",
    "y = a_true * x + b_true\n",
    "\n",
    "a = torch.zeros(1,requires_grad=True) # les objets que l'on va faire converger vers les valeurs recherchées\n",
    "b = torch.zeros(1,requires_grad=True) # requires_grad => un champs gradient est attaché à l'objet crée\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    y_est = a*x + b # je connais x, j'estime y / paramètres estimés courants \n",
    "                    # cette ligne crée un graphe de calcul entre x et y_est mettant en jeu a et b \n",
    "\n",
    "    loss = torch.mean((y_est-y)**2) # je calcule l'erreur entre l'estimation et les valeurs observées\n",
    "    \n",
    "    loss.backward() # je différencie la fonction de perte\n",
    "                    # cela entraine la différentiation automatique de tout le graphe de calcul\n",
    "                    # le gradient est mis à jour dans toutes les variables du graphe / requires_grad = True\n",
    "    \n",
    "    print(f'Itération {i}:')\n",
    "    print(f\"a: {a.item():.4f}, b: {b.item():.4F}, loss: {loss.item():.4f},  grad a: {a.grad.item():.4f}, grad b: {b.grad.item():.4f}\")\n",
    "   \n",
    "    with torch.no_grad(): #je vais effectuer des opérations sur des objets attachés au graphe de calcul \n",
    "                          # mais je ne veux pas que ces opérations entrent dans l'optimisation des paramètres\n",
    "        \n",
    "        a -=  0.1*a.grad # descente de gradient de pas 0.1\n",
    "        b -=  0.1*b.grad\n",
    "        \n",
    "        a.grad.zero_() # je remets à 0 tous les champs gradient des objets\n",
    "        b.grad.zero_()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ee4c4",
   "metadata": {},
   "source": [
    "# Définition d'un réseau de neurones \n",
    "\n",
    "## Complètement à la main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonModeleQuiTorche(torch.nn.Module):\n",
    "    def __init__(self,delta_chan=4,verbose=False):\n",
    "        if verbose:\n",
    "            self.print = print\n",
    "        else:\n",
    "            self.print = lambda x:None\n",
    "        self.print('Initialisation classe mère \\n')\n",
    "        torch.nn.Module.__init__(self) \n",
    "        \n",
    "        self.print('\\n Initialisation classe courante \\n')\n",
    "        self.delta_chan=delta_chan\n",
    "        self.learnable_param = torch.nn.Parameter(torch.rand([1,delta_chan,1]))\n",
    "        self.not_learnable_param = torch.rand((1,delta_chan,1))\n",
    "\n",
    "    def __setattr__(self,name,value):\n",
    "        super().__setattr__(name,value)\n",
    "        self.print(f'Enregistrement de: {name} à la valeur {value}')\n",
    "        \n",
    "    def forward(self,x): \n",
    "        #x is [B,input_chan,T]\n",
    "        # output is [B,self.output_chan,T]\n",
    "        x_reduced = torch.mean(x , axis = 1 , keepdim=True)\n",
    "        x_duplicated = torch.tile(x_reduced , dims = (1, self.delta_chan, 1))\n",
    "\n",
    "        y0 = self.learnable_param *x_duplicated \n",
    "        y1 = y0+ self.not_learnable_param\n",
    "\n",
    "        y2 = torch.abs(y1)\n",
    "        \n",
    "        y3 = torch.concat([x, y2], axis=1)\n",
    "        return y3\n",
    "    \n",
    "    def __call__(self,x):\n",
    "    # Défini dans la classe mère\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_modele=MonModeleQuiTorche(delta_chan=4, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.rand(5,1,100)\n",
    "y = mon_modele(x)\n",
    "z = mon_modele(y)\n",
    "print(f'Input shape {x.shape}, output shape {y.shape}, second output shape {z.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f219586",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Paramètres du modèle : \\n {mon_modele._parameters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd46f0b",
   "metadata": {},
   "source": [
    "## En enchaînant des couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55293b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_modele_sequentiel = torch.nn.Sequential( \n",
    "  MonModeleQuiTorche(4),\n",
    "  MonModeleQuiTorche(5),\n",
    "  MonModeleQuiTorche(6)  )\n",
    "\n",
    "mon_modele_sequentiel(torch.rand(5,1,100)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b573aa",
   "metadata": {},
   "source": [
    "### Quand il y en a beaucoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44022367",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = torch.nn.Sequential(*[MonModeleQuiTorche(4+i) for i in range(10)])\n",
    "print(mlp((torch.rand(5,1,100))).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d8d93",
   "metadata": {},
   "source": [
    "## Couches linéaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4abbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(in_features=40,\n",
    "                         out_features=100\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad515649",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([10, 40])\n",
    "print(f'Input shape is {x.shape}')\n",
    "y = linear(x)\n",
    "print(f'Output shape is {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nombre de paramètres à apprendre :  {sum(p.numel() for p in linear.parameters() if p.requires_grad)}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f259a108",
   "metadata": {},
   "source": [
    "### Couches linéaires sur un signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dbfef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([10, 4, 10]) #[B, C, T]\n",
    "print(f'Input shape is {x.shape}')\n",
    "x_flat = torch.flatten(x, start_dim = 1 , end_dim=2)\n",
    "print(f'x flatten shape is : {x_flat.shape}')\n",
    "y = linear(x_flat)\n",
    "print(f'Output shape is {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f9c92",
   "metadata": {},
   "source": [
    "## Couches de convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = torch.nn.Conv1d(in_channels=4,  # entrée [B,1,T]\n",
    "                       out_channels=10, # sortie [B,10,T']\n",
    "                       kernel_size=11, # préférer les nombres impairs \n",
    "                       stride=1,       # T' = T//2\n",
    "                       padding='same', # idem (kernel_size-1)//2 \n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([10, 4, 100]) #[B, C, T]\n",
    "print(f'Input shape is {x.shape}')\n",
    "\n",
    "y = conv(x)\n",
    "print(f'Output shape is {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nombre de paramètres à apprendre :  {sum(p.numel() for p in conv.parameters() if p.requires_grad)}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b270a",
   "metadata": {},
   "source": [
    "### Convolutions séparables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0841790",
   "metadata": {},
   "outputs": [],
   "source": [
    "depthwise = torch.nn.Conv1d(in_channels=4,  # entrée [B,1,T]\n",
    "                           out_channels=4,  # sortie [B,4,T]\n",
    "                           groups= 4,       # correspond à in_channels\n",
    "                           kernel_size=11,  # préférer les nombres impairs \n",
    "                           stride=1,  \n",
    "                           padding='same'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointwise = torch.nn.Conv1d(in_channels=4,  # entrée [B,1,T]\n",
    "                           out_channels=10,  # sortie [B,4,T]\n",
    "                           kernel_size=1,   # préférer les nombres impairs \n",
    "                           stride=1, \n",
    "                           padding='same'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35336750",
   "metadata": {},
   "outputs": [],
   "source": [
    "separable_convolution = torch.nn.Sequential(depthwise,\n",
    "                                            pointwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([10, 4, 100]) #[B, C, T]\n",
    "print(f'Input shape is {x.shape}')\n",
    "\n",
    "y = separable_convolution(x)\n",
    "print(f'Output shape is {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e050754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nombre de paramètres à apprendre :  {sum(p.numel() for p in separable_convolution.parameters() if p.requires_grad)}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e59b2",
   "metadata": {},
   "source": [
    "## Couches récurrentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent = torch.nn.RNN(input_size=6, # x is [B, T , input_size]\n",
    "                        hidden_size =15, # h is [B,T, hidden_size]\n",
    "                        num_layers =1, # par défaut\n",
    "                        batch_first=True,  # pour que la première dimension soit bien le batch\n",
    "                        bidirectional= False, # par défaut\n",
    "                         bias= False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49291ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.rand([20, 100,6])\n",
    "h , h_layers_end = recurrent(x)\n",
    "print(h.shape)\n",
    "print(h_layers_end.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddaac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nombre de paramètres à apprendre :  {sum(p.numel() for p in recurrent.parameters() if p.requires_grad)}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47920dd1",
   "metadata": {},
   "source": [
    "###  Réseau récurrent avec deux couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c4c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent = torch.nn.RNN(input_size=6, # x is [B, T , input_size]\n",
    "                        hidden_size =15, # h is [B,T, hidden_size]\n",
    "                        num_layers =4, # h de la couche 0 devient le x de la couche 1 etc.\n",
    "                        batch_first=True,  # pour que la première dimension soit bien le batch\n",
    "                        bidirectional= False, # valeur par défaut\n",
    "                        bias= False\n",
    "\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3981ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.rand([20, 100,6])\n",
    "h , h_layers_end = recurrent(x)\n",
    "print(h.shape)\n",
    "print(h_layers_end.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a09488",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nombre de paramètres à apprendre :  {sum(p.numel() for p in recurrent.parameters() if p.requires_grad)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee0b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9a122d0",
   "metadata": {},
   "source": [
    "###  Réseau récurrent bidirectionnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db232a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent = torch.nn.RNN(input_size=6, # x is [B, T , input_size]\n",
    "                        hidden_size =15, # h is [B,T, hidden_size]\n",
    "                        num_layers =7, # h de la couche 0 devient le x de la couche 1 etc.\n",
    "                        batch_first=True,  # pour que la première dimension soit bien le batch\n",
    "                        bidirectional= True, # cf Algorithme forward backward\n",
    "                         bias = False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d79f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.rand([20, 100,6])\n",
    "h , h_layers_end = recurrent(x)\n",
    "print(h.shape)\n",
    "print(h_layers_end.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac7720",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nombre de paramètres à apprendre :  {sum(p.numel() for p in recurrent.parameters() if p.requires_grad)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6834d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(6+15)*15*2 + 6*(15*2 + 15 )*(15)*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0faad",
   "metadata": {},
   "source": [
    "# Pipeline d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path_to_data):\n",
    "        ...\n",
    "    def __len__(self): #returns int\n",
    "        ...\n",
    "    def __getitem__(self,i): #returns (data_i,label_i)\n",
    "        ...\n",
    "\n",
    "dataset = MyDataset(...)\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=10, \n",
    "                        shuffle=True\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86990f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class metric_logger:\n",
    "    \n",
    "    def __init__(self,...):\n",
    "        ...\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        ... \n",
    "        \n",
    "    def update_metrics(self, batch_x,batch_y_true,batch_y_pred):\n",
    "        ...\n",
    "    \n",
    "        return {'metric0':...,\n",
    "               'metric1':...\n",
    "               }\n",
    "    \n",
    "    def log(self):\n",
    "        ...\n",
    "\n",
    "device = 'cpu' # set so 'cuda:xx' if you have a GPU, xx is GPU index\n",
    "model = ... \n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "metric_logger_train = metric_logger(...)\n",
    "metric_logger_valid = metric_logger(...)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    metric_logger_train.reset()\n",
    "    \n",
    "    for batch_x,batch_y in dataloader_train:\n",
    "        \n",
    "        batch_x.to(device)\n",
    "        batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_y_predicted = model(batch_x)\n",
    "        \n",
    "        l = loss(batch_y_predicted, batch_y)\n",
    "        \n",
    "        metric_logger_train.log(batch_x,batch_y,batch_y_predicted)\n",
    "        \n",
    "        l.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    for batch_x,batch_y in dataloader_valid:\n",
    "        \n",
    "        batch_x.to(device)\n",
    "        batch_y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_y_predicted = model(batch_x)  \n",
    "            \n",
    "        metric_logger_valid.log(batch_x,batch_y,batch_y_predicted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
