{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# <b>ALTEGRAD 2023\n",
    "# Lab 6: Deep Learning for Graphs (2/2)\n",
    "# MARENGO Matteo | matteo.marengo@ens-paris-saclay.fr\n",
    "# <b>PART 1 - Graph Neural Networks for Graph-Level Tasks\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Graph level means that each sample is a graph and not a node</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>1. Dataset Generation</b>\n",
    "The dataset will contain two types of graphs:\n",
    "- instances of the $G(n,0.2)$ Erdos-Rényi random model\n",
    "- instances of the $G(n,0.4)$ Erdos-Rényi random model\n",
    "- $n \\in [10,20]$\n",
    "\n",
    "In the $G(n,p)$ model, a graph is constructed by connecting nodes randomly.\n",
    "### <b>TASK 1</b>\n",
    "Generate 50 graphs using the $G(n,0.2)$ model and 50 graphs using the $G(n,0.4)$ model.<br>\n",
    "<i>Hint: Use the fast_gnp_random_graph() function of NetworkX</i>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep Learning on Graphs - ALTEGRAD - Nov 2023\n",
    "utils.py - 1/3\n",
    "Matteo MARENGO - matteo.marengo@ens-paris-saclay.fr\n",
    "\"\"\"\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from random import randint\n",
    "\n",
    "def create_dataset():\n",
    "    Gs = list()\n",
    "    y = list()\n",
    "\n",
    "    ############## Task 1\n",
    "    # Generate the dataset\n",
    "    # 50 graphs using the G(n,p1) model with p1=0.2\n",
    "    # 50 graphs using the G(n,p2) model with p2=0.4\n",
    "    \n",
    "    ##################\n",
    "    p1 = 0.2\n",
    "    p2 = 0.4\n",
    "\n",
    "    for i in range(50):\n",
    "        n = randint(10,20)\n",
    "        Gs.append(nx.fast_gnp_random_graph(n,p1))\n",
    "        y.append(0)\n",
    "\n",
    "        n = randint(10,20)\n",
    "        Gs.append(nx.fast_gnp_random_graph(n,p2))\n",
    "        y.append(1)\n",
    "    ##################\n",
    "\n",
    "    return Gs, y\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse_coo_tensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> 2. Implementation of Graph Neural Network </b>\n",
    "### <b>TASK 2</b>\n",
    "GNN model that consists of:\n",
    "- two message passing layers\n",
    "- sum readout function\n",
    "- two fully-connected layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep Learning on Graphs - ALTEGRAD - Nov 2023\n",
    "models.py - 2/3\n",
    "Matteo MARENGO - matteo.marengo@ens-paris-saclay.fr\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, n_class):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, hidden_dim_3)\n",
    "        self.fc4 = nn.Linear(hidden_dim_3, n_class)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in, adj, idx):\n",
    "        \n",
    "        ############## Task 2\n",
    "    \n",
    "        ##################\n",
    "        # a message passing layer with h1 hidden units followed by a ReLU activation function\n",
    "        x = self.fc1(x_in)\n",
    "        x = self.relu(torch.mm(adj, x))\n",
    "\n",
    "        # a message passing layer with h2 hidden units followed by a ReLU activation function\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(torch.mm(adj, x))\n",
    "\n",
    "        ##################\n",
    "        # a readout function that computes the sum of the node embeddings for each graph in the batch\n",
    "        idx = idx.unsqueeze(1).repeat(1, x.size(1))\n",
    "        out = torch.zeros(torch.max(idx)+1, x.size(1), device=x_in.device)\n",
    "        out = out.scatter_add_(0, idx, x) \n",
    "        \n",
    "        ##################\n",
    "        # your code here #\n",
    "        # a fully connected layer with h3 hidden units \n",
    "        out = self.relu(self.fc3(out))\n",
    "        # a fully connected layer with nclass hidden units \n",
    "        out = self.fc4(out)\n",
    "\n",
    "        ##################\n",
    "\n",
    "        return F.log_softmax(out, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> 3. Graph Classification\n",
    "### <b>TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ..\\c10\\cuda\\CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n class: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.7542 acc_train: 0.6000 time: 0.1476s\n",
      "Epoch: 0011 loss_train: 0.5967 acc_train: 0.7444 time: 0.0782s\n",
      "Epoch: 0021 loss_train: 0.3640 acc_train: 0.7889 time: 0.0723s\n",
      "Epoch: 0031 loss_train: 0.3258 acc_train: 0.8111 time: 0.0733s\n",
      "Epoch: 0041 loss_train: 0.3564 acc_train: 0.8000 time: 0.0728s\n",
      "Epoch: 0051 loss_train: 0.3434 acc_train: 0.7889 time: 0.0852s\n",
      "Epoch: 0061 loss_train: 0.3208 acc_train: 0.8444 time: 0.0673s\n",
      "Epoch: 0071 loss_train: 0.2892 acc_train: 0.8556 time: 0.0728s\n",
      "Epoch: 0081 loss_train: 0.3230 acc_train: 0.8111 time: 0.0728s\n",
      "Epoch: 0091 loss_train: 0.2747 acc_train: 0.8667 time: 0.0764s\n",
      "Epoch: 0101 loss_train: 0.2619 acc_train: 0.8778 time: 0.0811s\n",
      "Epoch: 0111 loss_train: 0.2577 acc_train: 0.8889 time: 0.0734s\n",
      "Epoch: 0121 loss_train: 0.2204 acc_train: 0.9000 time: 0.0812s\n",
      "Epoch: 0131 loss_train: 0.2185 acc_train: 0.9000 time: 0.0831s\n",
      "Epoch: 0141 loss_train: 0.2072 acc_train: 0.9333 time: 0.0890s\n",
      "Epoch: 0151 loss_train: 0.2153 acc_train: 0.9222 time: 0.1022s\n",
      "Epoch: 0161 loss_train: 0.1844 acc_train: 0.9444 time: 0.0769s\n",
      "Epoch: 0171 loss_train: 0.1713 acc_train: 0.9444 time: 0.0867s\n",
      "Epoch: 0181 loss_train: 0.0993 acc_train: 0.9889 time: 0.0850s\n",
      "Epoch: 0191 loss_train: 0.0830 acc_train: 0.9667 time: 0.0802s\n",
      "Optimization finished!\n",
      "loss_test: 0.0109 acc_test: 1.0000 time: 0.1176s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deep Learning on Graphs - ALTEGRAD - Nov 2023\n",
    "gnn.py - 3/3\n",
    "Matteo MARENGO - matteo.marengo@ens-paris-saclay.fr\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# from models import GNN\n",
    "# Sfrom utils import create_dataset, sparse_mx_to_torch_sparse_tensor\n",
    "\n",
    "# Initializes device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 200\n",
    "batch_size = 8\n",
    "n_hidden_1 = 16\n",
    "n_hidden_2 = 32\n",
    "n_hidden_3 = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Generates synthetic dataset\n",
    "Gs, y = create_dataset()\n",
    "n_class = np.unique(y).size\n",
    "print('n class: {}'.format(n_class))\n",
    "\n",
    "# Splits the dataset into a training and a test set\n",
    "G_train, G_test, y_train, y_test = train_test_split(Gs, y, test_size=0.1)\n",
    "\n",
    "N_train = len(G_train)\n",
    "N_test = len(G_test)\n",
    "\n",
    "# Initializes model and optimizer\n",
    "model = GNN(1, n_hidden_1, n_hidden_2, n_hidden_3, n_class).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Trains the model\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for i in range(0, N_train, batch_size):\n",
    "        adj_batch = list()\n",
    "        idx_batch = list()\n",
    "        y_batch = list()\n",
    "\n",
    "        ############## Task 3\n",
    "        \n",
    "        ##################\n",
    "        # your code here #\n",
    "\n",
    "        for j in range(i, min(N_train, i+batch_size)):\n",
    "            n = G_train[j].number_of_nodes()\n",
    "            adj_batch.append(nx.adjacency_matrix(G_train[j]) + sp.identity(n))\n",
    "            idx_batch.extend([j-i]*n)\n",
    "            y_batch.append(y_train[j])\n",
    "        # create a sparse block diagonal matrix from the adjacency matrices of the graphs in the batch\n",
    "        adj_batch = sp.block_diag(adj_batch)\n",
    "        # a matrix of size (N,1) where N is the number of nodes in the batch\n",
    "        # it countains thefeatures of the nodes in the batch\n",
    "        # here we use constant features equal to 1\n",
    "        features_batch = np.ones((adj_batch.shape[0],1))\n",
    "\n",
    "        # convert the numpy arrays to a torch sparse tensor\n",
    "        adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch).to(device)\n",
    "        features_batch = torch.FloatTensor(features_batch).to(device)\n",
    "        idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "        y_batch = torch.LongTensor(y_batch).to(device)\n",
    "        ##################\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(features_batch, adj_batch, idx_batch)\n",
    "        loss = loss_function(output, y_batch)\n",
    "        train_loss += loss.item() * output.size(0)\n",
    "        count += output.size(0)\n",
    "        preds = output.max(1)[1].type_as(y_batch)\n",
    "        correct += torch.sum(preds.eq(y_batch).double())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(train_loss / count),\n",
    "              'acc_train: {:.4f}'.format(correct / count),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "        \n",
    "print('Optimization finished!')\n",
    "\n",
    "# Evaluates the model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "for i in range(0, N_test, batch_size):\n",
    "    adj_batch = list()\n",
    "    idx_batch = list()\n",
    "    y_batch = list()\n",
    "\n",
    "    ############## Task 3\n",
    "    \n",
    "    ##################\n",
    "    # your code here #\n",
    "\n",
    "    for j in range(i, min(N_train, i+batch_size)):\n",
    "        # extract the adjacency matrix of the j-th graph in the batch\n",
    "        n = G_train[j].number_of_nodes()\n",
    "        adj_batch.append(nx.adjacency_matrix(G_train[j]) + sp.identity(n))\n",
    "        # a vector that contains the indices of the graphs in the batch\n",
    "        idx_batch.extend([j-i]*n)\n",
    "        # a vector that contains the class labels of the nodes in the batch\n",
    "        y_batch.append(y_train[j])\n",
    "\n",
    "    # create a sparse block diagonal matrix from the adjacency matrices of the graphs in the batch\n",
    "    adj_batch = sp.block_diag(adj_batch)\n",
    "    features_batch = np.ones((adj_batch.shape[0],1))\n",
    "\n",
    "    adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch).to(device)\n",
    "    features_batch = torch.FloatTensor(features_batch).to(device)\n",
    "    idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "    y_batch = torch.LongTensor(y_batch).to(device)\n",
    "    ##################\n",
    "\n",
    "    output = model(features_batch, adj_batch, idx_batch)\n",
    "    loss = loss_function(output, y_batch)\n",
    "    test_loss += loss.item() * output.size(0)\n",
    "    count += output.size(0)\n",
    "    preds = output.max(1)[1].type_as(y_batch)\n",
    "    correct += torch.sum(preds.eq(y_batch).double())\n",
    "\n",
    "print('loss_test: {:.4f}'.format(test_loss / count),\n",
    "      'acc_test: {:.4f}'.format(correct / count),\n",
    "      'time: {:.4f}s'.format(time.time() - t))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
